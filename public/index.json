[{"uri":"/","title":"Building a Stock Data Processing and Storage System using AWS Lambda, DynamoDB, S3, and Snowflake","tags":[],"description":"","content":"Building a Stock Data Processing and Storage System using AWS Lambda, DynamoDB, S3, and Snowflake Overall This project builds an automated stock data processing and storage system using AWS services and Snowflake. It retrieves Microsoft stock prices via an API using AWS Lambda and stores the data in Amazon DynamoDB. Another Lambda function processes and converts the data into CSV files, which are then stored in Amazon S3. Finally, the data is loaded into Snowflake for further analysis and reporting. This system leverages AWS Lambda, DynamoDB, S3, and CloudWatch Events to ensure efficient data collection, transformation, and storage.\nContent Introduction Preparation Collecting and Storing Data in DynamoDB Transforming and Storing Data as CSV in S3 Loading Data from S3 to Snowflake Clean up resources "},{"uri":"/2-prerequiste/2.1-createec2/","title":"Create IAM User","tags":[],"description":"","content":"Create IAM User In this step, we will need to create an IAM User account to use for this lab exercise.\n1. In the AWS Management Console, search for the IAM service and select it.\n2. In the IAM Dashboard, select Users from the left menu and then click Create user.\n3. Fill in the information.\nUsername: user_fcj_2024 Select Provide user access to the AWS Management Console Choose custom password and enter the password for the account. Optionally, you can require the user to create a new password when logging in, or not. Here, the user can log in directly without creating a new password. Select Next. 4. Attach the necessary permissions for the User.\nSelect Attach policies directly. Select the following permissions: AmazonDynamoDBFullAccess, AmazonS3FullAccess, AWSLambda_FullAccess, AmazonEventBridgeFullAccess, IAMFullAccess Select next. 5. Review the information added.\nClick create user.\nThe user has been successfully created, and you can download the CVS file with the user information.\nYou can use the URL link to log in and use the created account. "},{"uri":"/4-s3log/4.1-updateiamrole/","title":"Create S3 Bucket","tags":[],"description":"","content":"Create S3 Bucket 1. In the AWS Management Console, search for the S3 service and select it.\n2. Create an S3 Bucket\nSelect Create bucket In the Bucket name field, enter data-stock-prices-01 Leave the other options at their default settings. Click Create bucket The bucket will be created successfully. 3. Add a folder to store data.\nUnder Objects, click Create folder Folder name: snowflake Click Create folder The folder is created successfully. The name of the S3 bucket must be unique. If the name already exists, try adding a letter or number at the end.\n"},{"uri":"/3-accessibilitytoinstances/3.1-public-instance/","title":"Create table in DynamoDB","tags":[],"description":"","content":"Creating a Table in DynamoDB 1. In the AWS Management Console, search for the DynamoDB service and select it.\nClick Create table. 2. Information\nTable name: Enter stock-prices\nPartition key: symbol\nSort key: timestamp\nClick Create table.\nThe table has been successfully created. The table name must be unique.\n"},{"uri":"/1-introduce/","title":"Introduction","tags":[],"description":"","content":"Amazon DynamoDB Amazon DynamoDB is a fully managed, serverless NoSQL key-value database designed to support high-performance applications at any scale. It offers built-in security, continuous backups, multi-region replication, in-memory caching, and data import/export tools.\nAvailability, durability, and fault tolerance are built-in and cannot be disabled, eliminating the need to design these functionalities for your application.\nDynamoDB is optimized for high-performance applications operating at an internet scale, far exceeding the capabilities of traditional relational databases. With over a decade of pioneering innovation, DynamoDB provides limitless scalability, consistent single-digit millisecond performance, and up to 99.999% availability.\nAmazon Simple Storage Service (Amazon S3) Amazon Simple Storage Service (Amazon S3) is an object storage service that provides industry-leading scalability, data availability, security, and performance.\nOrganizations of all sizes and industries use Amazon S3 to store and protect any amount of data for nearly any use case, including data lakes, cloud-native applications, and mobile apps.\nWith cost-effective storage classes and easy-to-use management features, Amazon S3 allows you to ooptimize costs, organize data efficiently, and configure fine-grained access controls to meet business, organizational, and compliance requirements.\nAWS Lambda AWS Lambda is a serverless compute service that runs your code in response to events while automatically managing the underlying computing resources. It provides a fast, scalable, and cost-efficient way to build modern applications without provisioning or managing servers.\nLambda allows you to execute code in response to triggers such as S3 events, DynamoDB updates, API requests, and CloudWatch events, making it an integral part of serverless architectures.\nSnowflake Snowflake is a cloud-based data platform that enables organizations to store, manage, and analyze large-scale datasets with high performance, scalability, and security. It offers a multi-cluster architecture, allowing users to run concurrent analytical workloads without performance degradation.\nKey Advantages\nUnlimited Scalability â€“ Snowflake dynamically scales resources up or down based on demand without service interruption.\nHigh Performance â€“ Multi-cluster computing optimizes query processing, enhancing speed and efficiency.\nSecurity \u0026amp; Compliance â€“ Features advanced security mechanisms such as data encryption, access control, and compliance with global security standards**.\nEase of Use â€“ Snowflakeâ€™s intuitive interface simplifies data management and analysis, even for users without deep technical expertise.\nSnowflake is an AWS Partner with competencies in Data Analytics, Machine Learning, and Retail, making it a powerful choice for cloud-based data warehousing and analytics. ðŸš€\n"},{"uri":"/2-prerequiste/2.2-createiamrole/","title":"Create Snowflake Account","tags":[],"description":"","content":"Create Snowflake Account In this section, we will create a Snowflake account using the free trial for the first 30 days to serve the purpose of this lab exercise.\n1. Access the Snowflake website\nVisit the link: Snowflake\nSelect: Start for free\nFill in the necessary information.\nSelect next. You can skip the remaining steps.\n2. Check the information sent to your Gmail.\nAfter successfully creating the account, you will receive an email from Snowflake to the email you registered with.\nSelect Click to activate Fill in the necessary information. Successfully log in to the Snowflake homepage. "},{"uri":"/2-prerequiste/","title":"Preparation ","tags":[],"description":"","content":"\rYou will need an AWS and Snowflake account to complete this hands-on lab..\nTo upload data from S3 to Snowflake, you can create a free trial account for 30 days on Snowflake\u0026rsquo;s homepage.\nStock Data Source The stock data is fetched directly from the website Alphavantage and is updated continuously.\nContent Create IAM User Create Snowflake Account "},{"uri":"/4-s3log/4.2-creates3bucket/","title":"Store Data into S3 Bucket","tags":[],"description":"","content":"Save Data to S3 Bucket Create Lambda Function 1. In the AWS Management Console, search for the Lambda service and select it.\n2. Create a function in Lambda\nSelect Create function Function name: DTToSnowflake Runtime: Python 3.10 Architecture: x86_64 Click Create function Set Up Lambda Function 1. Add a layer.\nNavigate to the Layers section and click Add a layer Layer source: AWS layers AWS layers: AWSSDKPandas-Python310 Version: 23 Make sure to choose the latest version.\n2. Add permissions to the function.\nIn the Configuration section, select Permissions on the left Click on the role name. In the Permissions section, you should see Permissions policies Click Add permissions â†’ Attach policies Add the AmazonDynamoDBFullAccess, AmazonS3FullAccess permissions Click Add permissions 4. Add Trigger.\nIn the Configuration section, select Trigger on the left. Click Add trigger Search for DynamoDB and select it. DynamoDB table: choose the table name created in DynamoDB from step 3. Click Add 5. Fetch data from the stock_prices table in DynamoDB and save it to the S3 Bucket created.\nGo to the Code section in the options bar. Add the SourceCode to the Code source Click Test â†’ Create new test event â†’ fill in the required details and click Save Run the code. Result after running: Results in the S3 Bucket Access the S3 Bucket In the left menu, click General purpose buckets â†’ Select the previously created table â†’ Navigate to the folder you created. Stock data has been processed into .csv files by the respective date and stored in the created S3 Bucket. "},{"uri":"/3-accessibilitytoinstances/3.2-private-instance/","title":"Storing Data in DynamoDB","tags":[],"description":"","content":"Storing Data into DynamoDB Create Lambda Function 1. In the AWS Management Console, search for the Lambda service and select it.\n2. Create a function in Lambda\nClick Create function Function name: fetch_code Runtime: Python 3.10 Architecture: x86_64 Click Create function Set Up Lambda Function 1. Add a layer.\nNavigate to the Layers section and click Add a layer Layer source: AWS layers AWS layers: AWSSDKPandas-Python310 Version: 23 Select the latest version in the version section.\n2. Adjust timeout.\nGo to the Configuration section. Under General configuration, click Edit. Set the Timeout to 15 seconds. 3. Add permissions for the function.\nIn the Configuration section, select Permissions on the left. Click on the role name. In the Permissions section, find Permissions policies. Click Add permissions - Attach policies. Add the permission AmazonDynamoDBFullAccess. Click Add permissions. 4. Insert data from Alphavantage into the table created in DynamoDB.\nGo to the Code section in the options bar. Add the SourceCode in the Code source area. Click Test - Create new test event, enter the necessary information, and click Save. Execute the code. Result after execution. Result in DynamoDB. Go to DynamoDB. Select Tables in the left-hand options bar - Click the table you created earlier. In the table, click Explore table items - Click the Run button. The data has been fetched from Alphavantage and stored in the stock_prices table created earlier in DynamoDB. "},{"uri":"/3-accessibilitytoinstances/","title":"Collecting and Storing Data in DynamoDB","tags":[],"description":"","content":"In this step, we will connect to our EC2 servers, located in both the public and private subnets.\nContent 3.1. Create table in DynamoDB 3.2. Storing Data in DynamoDB\n"},{"uri":"/4-s3log/","title":"Transforming and Storing Data as CSV in S3","tags":[],"description":"","content":"In this step, we will use the data from the stock_prices table created in DynamoDB in Step 3, convert it into .csv files, and store them in an S3 bucket.\nContent: Create S3 Bucket Store Data into S3 Bucket "},{"uri":"/5-portfwd/","title":"Loading Data from S3 to Snowflake","tags":[],"description":"","content":"\rThis section requires a Snowflake account, so make sure you have one.\nIn the previous section, we stored stock data as .csv files by date in an S3 Bucket. Now, we will load it into Snowflake for easier visualization, usage, and management.\nCreate a Database in Snowflake 1. Go to Snowflake\nLog in with your existing account. 2. Select Create â†’ SQL worksheets\nRun the following commands: CREATE DATABASE FCJ_STOCK_PRICES; USE DATABASE FCJ_STOCK_PRICES; The Database is successfully created. 3. Create a Stage in Snowflake\nCREATE STAGE my_stage URL = \u0026#39;s3://data-stock-prices-01/snowflake/\u0026#39; CREDENTIALS = (AWS_KEY_ID = \u0026#39;\u0026lt;\u0026lt;your aws key id\u0026gt;\u0026gt;\u0026#39; AWS_SECRET_KEY = \u0026#39;your aws secret key\u0026#39;); For this step, go back to AWS with an admin account and retrieve the AWS_KEY_ID and AWS_SECRET_KEY associated with the AWS account storing the S3 Bucket created in the previous step.\nTo obtain these credentials, navigate to IAM: Select Users â†’ Go to Security credentials â†’ Under Access keys, you will find your AWS_KEY_ID and AWS_SECRET_KEY. (Alternatively, you can create a new access key and use the newly generated credentials for the above command.)\n4. Create the stock_prices table in Snowflake\nRun the following SQL command: CREATE OR REPLACE TABLE stock_prices( low VARCHAR(128), symbol VARCHAR(128), timestamp VARCHAR(128), open VARCHAR(128), volume VARCHAR(128), high VARCHAR(128), close VARCHAR(128), date VARCHAR(128) ); The stock_prices table is successfully created in Snowflake. 5. Load data into the table\nRun the following command: COPY INTO stock_prices FROM @my_stage FILE_FORMAT = (TYPE = \u0026#39;CSV\u0026#39; FIELD_OPTIONALLY_ENCLOSED_BY = \u0026#39;\u0026#34;\u0026#39; SKIP_HEADER = 1) ON_ERROR = \u0026#39;SKIP_FILE\u0026#39;; Data loading is successful. Use the command SELECT * FROM stock_prices; to verify the data. The data has been successfully loaded into the table. Next, we will configure EventBridge to continuously update data in S3 Bucket in real time.\nAdd EventBridge (CloudWatch Events) 1. Go back to AWS and select Lambda.\nClick on Functions. Select the function fetch_code. 2. Add an EventBridge (CloudWatch Events) trigger\nGo to Configuration â†’ Trigger Click Add trigger Select a source: EventBridge CloudWatch Events Rule: Create a new rule Rule name: every_days Schedule expression: rate(1 day) Click Add With this setup, data will be continuously updated and inserted into DynamoDB and S3 on a daily basis.\n"},{"uri":"/6-cleanup/","title":"Clean up resources","tags":[],"description":"","content":"We will take the following steps to delete the resources we created in this exercise.\nCleanup Process: Removing AWS and Snowflake Resources To fully remove the resources created during this project, follow the structured cleanup process below.\n1. Delete the Snowflake Database Log in to your Snowflake account. Navigate to Data â†’ Databases. Locate the database created earlier. Run the following command in the SQL worksheet: DROP DATABASE FCJ_STOCK_PRICES; Confirm that the database has been successfully removed. 2. Delete IAM User Open the AWS Management Console and search for IAM. Navigate to Users. Locate and select the User that were created for this project. Click Delete and confirm the removal of each user. 3. Delete Lambda Functions Go to AWS Lambda in the AWS Management Console. Under Functions, find the Lambda functions created for this project. Select the function, click Actions, then choose Delete. Confirm deletion when prompted. 4. Delete S3 Bucket Navigate to Amazon S3 in the AWS Management Console. Locate the bucket created for storing stock price data. Select the bucket and click Empty. Type \u0026ldquo;permanently delete\u0026rdquo; to confirm and empty the bucket. After the bucket is emptied, click Delete and confirm its removal. 5. Delete DynamoDB Table Open the AWS Management Console and search for DynamoDB. Navigate to Tables and locate the table created in this project. Select the table, then choose Delete. Confirm the deletion and wait for the table removal process to complete. Final Check Ensure that all AWS resources (IAM roles, Lambda functions, S3 buckets, and DynamoDB tables) have been deleted. Verify that the Snowflake database is no longer present. "},{"uri":"/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"/tags/","title":"Tags","tags":[],"description":"","content":""}]