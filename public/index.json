[
{
	"uri": "https://vuthibichngoc.github.io/workshop_awsfcj_2024/",
	"title": "Building a Stock Data Processing and Storage System using AWS Lambda, DynamoDB, S3, and Snowflake",
	"tags": [],
	"description": "",
	"content": "Building a Stock Data Processing and Storage System using AWS Lambda, DynamoDB, S3, and Snowflake Overall This project builds an automated stock data processing and storage system using AWS services and Snowflake. It retrieves Microsoft stock prices via an API using AWS Lambda and stores the data in Amazon DynamoDB. Another Lambda function processes and converts the data into CSV files, which are then stored in Amazon S3. Finally, the data is loaded into Snowflake for further analysis and reporting. This system leverages AWS Lambda, DynamoDB, S3, and CloudWatch Events to ensure efficient data collection, transformation, and storage.\nContent Introduction Preparation Collecting and Storing Data in DynamoDB Transforming and Storing Data as CSV in S3 Loading Data from S3 to Snowflake Clean up resources "
},
{
	"uri": "https://vuthibichngoc.github.io/workshop_awsfcj_2024/2-prerequiste/2.1-createec2/",
	"title": "Create IAM User",
	"tags": [],
	"description": "",
	"content": "Create IAM User In this step, we will need to create an IAM User account to use for this lab exercise.\n1. In the AWS Management Console, search for the IAM service and select it.\n2. In the IAM Dashboard, select Users from the left menu and then click Create user.\n3. Fill in the information.\nUsername: user_fcj_2024 Select Provide user access to the AWS Management Console Choose custom password and enter the password for the account. Optionally, you can require the user to create a new password when logging in, or not. Here, the user can log in directly without creating a new password. Select Next. 4. Attach the necessary permissions for the User.\nSelect Attach policies directly. Select the following permissions: AmazonDynamoDBFullAccess, AmazonS3FullAccess, AWSLambda_FullAccess, AmazonEventBridgeFullAccess, IAMFullAccess. Select Next. 5. Review the information added.\nClick Create user.\nThe user has been successfully created, and you can download the CVS file with the user information.\nYou can use the URL link to log in and use the created account. "
},
{
	"uri": "https://vuthibichngoc.github.io/workshop_awsfcj_2024/4-s3log/4.1-updateiamrole/",
	"title": "Create S3 Bucket",
	"tags": [],
	"description": "",
	"content": "Create S3 Bucket 1. In the AWS Management Console, search for the S3 service and select it.\n2. Create an S3 Bucket\nSelect Create bucket In the Bucket name field, enter data-stock-prices-01 Leave the other options at their default settings. Click Create bucket The bucket will be created successfully. 3. Add a folder to store data.\nUnder Objects, click Create folder Folder name: snowflake Click Create folder The folder is created successfully. The name of the S3 bucket must be unique. If the name already exists, try adding a letter or number at the end.\n"
},
{
	"uri": "https://vuthibichngoc.github.io/workshop_awsfcj_2024/3-accessibilitytoinstances/3.1-public-instance/",
	"title": "Create table in DynamoDB",
	"tags": [],
	"description": "",
	"content": "Creating a Table in DynamoDB 1. In the AWS Management Console, search for the DynamoDB service and select it.\nClick Create table. 2. Information\nTable name: Enter stock-prices\nPartition key: symbol\nSort key: timestamp\nClick Create table.\nThe table has been successfully created. The table name must be unique.\n"
},
{
	"uri": "https://vuthibichngoc.github.io/workshop_awsfcj_2024/1-introduce/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Amazon DynamoDB Amazon DynamoDB is a fully managed, serverless NoSQL key-value database designed to support high-performance applications at any scale. It offers built-in security, continuous backups, multi-region replication, in-memory caching, and data import/export tools.\nAvailability, durability, and fault tolerance are built-in and cannot be disabled, eliminating the need to design these functionalities for your application.\nDynamoDB is optimized for high-performance applications operating at an internet scale, far exceeding the capabilities of traditional relational databases. With over a decade of pioneering innovation, DynamoDB provides limitless scalability, consistent single-digit millisecond performance, and up to 99.999% availability.\nAmazon Simple Storage Service (Amazon S3) Amazon Simple Storage Service (Amazon S3) is an object storage service that provides industry-leading scalability, data availability, security, and performance.\nOrganizations of all sizes and industries use Amazon S3 to store and protect any amount of data for nearly any use case, including data lakes, cloud-native applications, and mobile apps.\nWith cost-effective storage classes and easy-to-use management features, Amazon S3 allows you to ooptimize costs, organize data efficiently, and configure fine-grained access controls to meet business, organizational, and compliance requirements.\nAWS Lambda AWS Lambda is a serverless compute service that runs your code in response to events while automatically managing the underlying computing resources. It provides a fast, scalable, and cost-efficient way to build modern applications without provisioning or managing servers.\nLambda allows you to execute code in response to triggers such as S3 events, DynamoDB updates, API requests, and CloudWatch events, making it an integral part of serverless architectures.\nSnowflake Snowflake is a cloud-based data platform that enables organizations to store, manage, and analyze large-scale datasets with high performance, scalability, and security. It offers a multi-cluster architecture, allowing users to run concurrent analytical workloads without performance degradation.\nKey Advantages\nUnlimited Scalability â€“ Snowflake dynamically scales resources up or down based on demand without service interruption.\nHigh Performance â€“ Multi-cluster computing optimizes query processing, enhancing speed and efficiency.\nSecurity \u0026amp; Compliance â€“ Features advanced security mechanisms such as data encryption, access control, and compliance with global security standards**.\nEase of Use â€“ Snowflakeâ€™s intuitive interface simplifies data management and analysis, even for users without deep technical expertise.\nSnowflake is an AWS Partner with competencies in Data Analytics, Machine Learning, and Retail, making it a powerful choice for cloud-based data warehousing and analytics. ðŸš€\n"
},
{
	"uri": "https://vuthibichngoc.github.io/workshop_awsfcj_2024/2-prerequiste/2.2-createiamrole/",
	"title": "Create Snowflake Account",
	"tags": [],
	"description": "",
	"content": "Create Snowflake Account In this section, we will create a Snowflake account using the free trial for the first 30 days to serve the purpose of this lab exercise.\n1. Access the Snowflake website\nVisit the link: Snowflake\nSelect: Start for free\nFill in the necessary information.\nSelect next. You can skip the remaining steps.\n2. Check the information sent to your Gmail.\nAfter successfully creating the account, you will receive an email from Snowflake to the email you registered with.\nSelect Click to activate Fill in the necessary information. Successfully log in to the Snowflake homepage. "
},
{
	"uri": "https://vuthibichngoc.github.io/workshop_awsfcj_2024/2-prerequiste/",
	"title": "Preparation ",
	"tags": [],
	"description": "",
	"content": "\rYou will need an AWS and Snowflake account to complete this hands-on lab..\nTo upload data from S3 to Snowflake, you can create a free trial account for 30 days on Snowflake\u0026rsquo;s homepage.\nStock Data Source The stock data is fetched directly from the website Alphavantage and is updated continuously.\nContent Create IAM User Create Snowflake Account "
},
{
	"uri": "https://vuthibichngoc.github.io/workshop_awsfcj_2024/4-s3log/4.2-creates3bucket/",
	"title": "Store Data into S3 Bucket",
	"tags": [],
	"description": "",
	"content": "Save Data to S3 Bucket Create Lambda Function 1. In the AWS Management Console, search for the Lambda service and select it.\n2. Create a function in Lambda\nSelect Create function Function name: DTToSnowflake Runtime: Python 3.10 Architecture: x86_64 Click Create function Set Up Lambda Function 1. Add a Layer.\nNavigate to the Layers section and click Add a layer Layer source: AWS layers AWS layers: AWSSDKPandas-Python310 Version: 23 Make sure to choose the latest version.\n2. Add permissions to the function.\nIn the Configuration section, select Permissions on the left Click on the role name. In the Permissions section, you should see Permissions policies Click Add permissions â†’ Attach policies Add the AmazonDynamoDBFullAccess, AmazonS3FullAccess permissions Click Add permissions 4. Add Trigger.\nIn the Configuration section, select Trigger on the left. Click Add trigger Search for DynamoDB and select it. DynamoDB table: choose the table name created in DynamoDB from step 3. Click Add 5. Fetch data from the stock_prices table in DynamoDB and save it to the S3 Bucket created.\nGo to the Code section in the options bar. Add the SourceCode to the Code source Click Test â†’ Create new test event â†’ fill in the required details and click Save Click Deploy Run the code. Result after running: Results in the S3 Bucket Access the S3 Bucket In the left menu, click General purpose buckets â†’ Select the previously created table â†’ Navigate to the folder you created. Stock data has been processed into .csv files by the respective date and stored in the created S3 Bucket. "
},
{
	"uri": "https://vuthibichngoc.github.io/workshop_awsfcj_2024/3-accessibilitytoinstances/3.2-private-instance/",
	"title": "Storing Data in DynamoDB",
	"tags": [],
	"description": "",
	"content": "Storing Data into DynamoDB Create Lambda Function 1. In the AWS Management Console, search for the Lambda service and select it.\n2. Create a function in Lambda\nClick Create function Function name: fetch_code Runtime: Python 3.10 Architecture: x86_64 Click Create function Set Up Lambda Function 1. Add a Layer.\nNavigate to the Layers section and click Add a layer Layer source: AWS layers AWS layers: AWSSDKPandas-Python310 Version: 23 Select the latest version in the version section.\n2. Adjust timeout.\nGo to the Configuration section. Under General configuration, click Edit. Set the Timeout to 15 seconds. 3. Add permissions for the function.\nIn the Configuration section, select Permissions on the left. Click on the role name. In the Permissions section, find Permissions policies. Click Add permissions - Attach policies. Add the permission AmazonDynamoDBFullAccess. Click Add permissions. 4. Insert data from Alphavantage into the table created in DynamoDB.\nGo to the Code section in the options bar. Add the SourceCode in the Code source area. Click Test - Create new test event, enter the necessary information, and click Save. Click Deploy Execute the code. Result after execution. Result in DynamoDB. Go to DynamoDB. Select Tables in the left-hand options bar - Click the table you created earlier. In the table, click Explore table items - Click the Run button. The data has been fetched from Alphavantage and stored in the stock_prices table created earlier in DynamoDB. "
},
{
	"uri": "https://vuthibichngoc.github.io/workshop_awsfcj_2024/3-accessibilitytoinstances/",
	"title": "Collecting and Storing Data in DynamoDB",
	"tags": [],
	"description": "",
	"content": "In this step, we will retrieve stock data from the API and store it in a table that will be created in this step in DynamoDB.\nContent 3.1. Create table in DynamoDB 3.2. Storing Data in DynamoDB\n"
},
{
	"uri": "https://vuthibichngoc.github.io/workshop_awsfcj_2024/4-s3log/",
	"title": "Transforming and Storing Data as CSV in S3",
	"tags": [],
	"description": "",
	"content": "In this step, we will use the data from the stock_prices table created in DynamoDB in Step 3, convert it into .csv files, and store them in an S3 bucket.\nContent: Create S3 Bucket Store Data into S3 Bucket "
},
{
	"uri": "https://vuthibichngoc.github.io/workshop_awsfcj_2024/5-portfwd/5.4/",
	"title": "Adding EventBridge (CloudWatch Events)",
	"tags": [],
	"description": "",
	"content": "Adding EventBridge (CloudWatch Events) 1. Go back to AWS and select Lambda\nChoose Function Select the function fetch_code 2. Add an EventBridge (CloudWatch Events) trigger\nGo to Configuration - select Trigger Click Add trigger Select a source: EventBridge CloudWatch Events Rule: Create a new rule Rule name: every_days Schedule expression: rate(1 day) Click Add Once completed, the data will be updated and automatically sent to DynamoDB, S3, and Snowflake daily. Each day, you can check the newly updated stock information.\n"
},
{
	"uri": "https://vuthibichngoc.github.io/workshop_awsfcj_2024/5-portfwd/5.1/",
	"title": "Create IAM Role",
	"tags": [],
	"description": "",
	"content": "Create an IAM Role to allow Snowflake access to S3 1. In AWS Management Console, search for the IAM service and select it.\n2. At Step 01\nTrusted entity type: AWS account An AWS account: This account Next 3. At Step 02\nGrant this role the AmazonS3FullAccess permission Next 4. At Step 03\nRole name: snowflake-stock-prices Review the details Create role Remember to save the ARN of this role for the next step.\nThe next step will be to move the data from S3 to Snowflake.\n"
},
{
	"uri": "https://vuthibichngoc.github.io/workshop_awsfcj_2024/5-portfwd/5.3/",
	"title": "Data Charts in Snowflake",
	"tags": [],
	"description": "",
	"content": "Observing Data in Snowflake Select Chart on the result bar in Snowflake after running the query:\nSELECT * FROM stock_price_data; Here, you can create charts based on the data stored in the table. The sections include: Chart type, Data, and Appearance\n1. Chart type Chart type allows users to select different types of charts to visualize data.\n\u0026ndash; Line Chart\n\u0026ndash; Bar Chart\n\u0026ndash; Scatter Plot\n\u0026ndash; Heatgrid\n\u0026ndash; Scorecard\n2. Data Data refers to the information stored in a table or retrieved through a query. This serves as the source for chart visualization.\nIt allows you to select specific columns, aggregate values (sum, max, min, etc.), remove columns from the chart, or add new ones.\n3. Appearance Appearance relates to the visual representation of the chart. It does not affect the data but enhances readability and interpretation.\nExample: Adding column names and legend to the chart.\nBefore adding: After adding: Next, we will add EventBridge to ensure that data is continuously updated in real-time into the S3 Bucket and Snowflake.\n"
},
{
	"uri": "https://vuthibichngoc.github.io/workshop_awsfcj_2024/5-portfwd/",
	"title": "Loading Data from S3 to Snowflake",
	"tags": [],
	"description": "",
	"content": "In the previous section, we had stock data stored as daily .csv files in an S3 Bucket. Now, we will upload it to Snowflake for easier visualization, usage, and management.\nThis section requires a Snowflake account, so make sure you have one.\nContent: Create IAM Role Upload data to Snowflake Data Charts in Snowflake Adding EventBridge (CloudWatch Events) "
},
{
	"uri": "https://vuthibichngoc.github.io/workshop_awsfcj_2024/5-portfwd/5.2/",
	"title": "Upload data to Snowflake",
	"tags": [],
	"description": "",
	"content": "Create a Database in Snowflake 1. Go to Snowflake\nLog in with the created account. 2. Select Create - SQL worksheets\nRun the following commands: CREATE DATABASE STOCK_PRICES;\nUSE DATABASE STOCK_PRICES;\nDatabase created successfully. 3. Create the stock_prices_data table in Snowflake and load data into it.\na. Create the stock data table.\nCREATE OR REPLACE TABLE stock_prices_data(\rlow VARCHAR(128), symbol VARCHAR(128),\rtimestamp VARCHAR(128),\ropen VARCHAR(128),\rvolume VARCHAR(128),\rhigh VARCHAR(128),\rclose VARCHAR(128),\rdate VARCHAR(128)\r); Table created successfully in Snowflake. b. Create an Integration Object to connect with S3.\nAn Integration Object in Snowflake is an external connection object that allows Snowflake to communicate with other services such as S3.\nRun the following command to create an Integration Object to connect with S3.\ncreate or replace storage integration s3_int\rtype = external_stage\rstorage_provider = s3\renabled = true\rstorage_aws_role_arn = \u0026#39;\u0026lt;\u0026lt;Your role ARN\u0026gt;\u0026gt;\u0026#39;\rstorage_allowed_locations = (\u0026#39;s3://data-stock-prices-01/snowflake/\u0026#39;); Enter the ARN of the IAM Role created in the previous step to allow Snowflake to access S3, and modify the S3 path to match your bucket name.\nc. Check detailed information about Storage Integration.\nStorage Integration is an object in Snowflake used to securely and automatically connect with external storage services like AWS S3. Instead of providing an access key and secret key to access S3, Snowflake allows the use of AWS IAM Roles to grant access securely.\nRun the command:\nDESC INTEGRATION s3_int;\nto check detailed information about Storage Integration.\nSave the property_value of the following properties: STORAGE_AWS_IAM_USER_ARN, STORAGE_AWS_ROLE_ARN, STORAGE_AWS_EXTERNAL_ID\nGo back to IAM - select Roles - choose the role created in the previous step. Select Trust relationship - Edit trust policy Enter the property_value of STORAGE_AWS_ROLE_ARN saved earlier after \u0026ldquo;AWS\u0026rdquo; Then, continue by selecting Add condition\nFill in the following information: \u0026ndash; Condition key: sts:ExternalId\n\u0026ndash; Qualifier: Default\n\u0026ndash; Operator: StringEquals\n\u0026ndash; Value: Enter the property_value of STORAGE_AWS_EXTERNAL_ID saved earlier.\nAdd condition d. Create File Format to read CSV files\nRun the following command:\ncreate or replace file format csv_format\rtype = csv\rfield_delimiter = \u0026#39;,\u0026#39;\rskip_header = 1\rnull_if = (\u0026#39;NULL\u0026#39;, \u0026#39;null\u0026#39;)\rempty_field_as_null = true; e. Create External Stage to connect with S3\nExternal Stage is an external storage repository used by Snowflake to retrieve data from cloud storage services like Amazon S3, Google Cloud Storage, or Azure Blob Storage. It allows Snowflake to read and write data directly from/to external storage without needing to load data into Snowflake first.\nRun the following command to create an External Stage to connect with the S3 bucket created earlier.\ncreate or replace stage ext_csv_stage\rURL = \u0026#39;s3://data-stock-prices-01/snowflake/\u0026#39;\rSTORAGE_INTEGRATION = s3_int\rfile_format = csv_format; f. Create Pipe to automatically load data\nA Pipe in Snowflake is a mechanism to automate data loading from an External Stage (S3) into a table in Snowflake. It uses Snowpipe, a Snowflake service that automatically detects new files in cloud storage (e.g., S3) and loads data into the table immediately without requiring a manual COPY INTO command.\nGo back to IAM - Roles - select the role created in the previous step. Select Trust relationship - Edit trust policy Modify the content and enter the property_value of STORAGE_AWS_IAM_USER_ARN saved earlier after \u0026ldquo;AWS\u0026rdquo; Run the following command.\ncreate or replace pipe mypipe auto_ingest=true as\rcopy into stock_price_data\rfrom @ext_csv_stage\ron_error = CONTINUE; g. Execute the command show pipes to check pipe\nPlease save the information displayed in the notification_channel column.\nGo back to S3 Select the bucket used to store data that will be uploaded to Snowflake Navigate to Event notifications Click Create event notification In General configuration \u0026ndash; Event name: stock-price-event\nIn Event types \u0026ndash; Select All object create events\nIn Destination \u0026ndash; Destination: SQS queue\n\u0026ndash; SQS queue: enter the notification_channel information that was saved earlier.\nSave changes Event notification has been successfully created.\nGo back to Lambda Select Function Select the first function created to insert data into DynamoDB (in this case, the function is named fetch_code) Navigate to Code Select the Test event that was used before - choose Edit test event - then select Invoke 4. Results\nExecute the command select * from stock_price_data; to check if the data has been loaded from S3 into Snowflake The data from S3 has been successfully added to Snowflake.\nIn the next step, we will use Chart in Snowflake to create data charts on Snowflake to help viewers easily analyze and evaluate data.\n"
},
{
	"uri": "https://vuthibichngoc.github.io/workshop_awsfcj_2024/6-cleanup/",
	"title": "Clean up resources",
	"tags": [],
	"description": "",
	"content": "We will take the following steps to delete the resources we created in this exercise.\nCleanup Process: Removing AWS and Snowflake Resources To fully remove the resources created during this project, follow the structured cleanup process below.\n1. Delete the Snowflake Database Log in to your Snowflake account. Navigate to Data â†’ Databases. Locate the database created earlier. Run the following command in the SQL worksheet: DROP DATABASE STOCK_PRICES; Confirm that the database has been successfully removed. 2. Delete IAM User Open the AWS Management Console and search for IAM. Navigate to Users. Locate and select the User that were created for this project. Click Delete and confirm the removal of each user. 3. Delete Lambda Functions Go to AWS Lambda in the AWS Management Console. Under Functions, find the Lambda functions created for this project. Select the function, click Actions, then choose Delete. Confirm deletion when prompted. 4. Delete S3 Bucket Navigate to Amazon S3 in the AWS Management Console. Locate the bucket created for storing stock price data. Select the bucket and click Empty. Type \u0026ldquo;permanently delete\u0026rdquo; to confirm and empty the bucket. After the bucket is emptied, click Delete and confirm its removal. 5. Delete DynamoDB Table Open the AWS Management Console and search for DynamoDB. Navigate to Tables and locate the table created in this project. Select the table, then choose Delete. Confirm the deletion and wait for the table removal process to complete. Final Check Ensure that all AWS resources (IAM roles, Lambda functions, S3 buckets, and DynamoDB tables) have been deleted. Verify that the Snowflake database is no longer present. "
},
{
	"uri": "https://vuthibichngoc.github.io/workshop_awsfcj_2024/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://vuthibichngoc.github.io/workshop_awsfcj_2024/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]